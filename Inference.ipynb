{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnMwoCA7iium",
        "outputId": "b13f6f23-9d09-4a4f-87fa-1041af0b14c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Download NLTK stopwords (only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "# If not done already, download necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yzg9vURitx7",
        "outputId": "03227d0a-2e77-4f8b-f6e6-ef9d03d9e9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.remove('not')  # Keep 'not' for better negation handling\n",
        "\n",
        "# POS tag converter\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Clean + lemmatize function\n",
        "def clean_and_lemmatize(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r'\\bRT\\b', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub('[^a-zA-Z\\s]', ' ', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    lemmatized_words = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "        for word, tag in tagged_tokens\n",
        "        if word not in stop_words\n",
        "    ]\n",
        "    return ' '.join(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "gCB5hckZiyQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "nd7kflYNi7Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path='/content/drive/MyDrive/FreelanceProjects/StudentSelection'\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    hidden_dropout_prob=0.3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbeRD0Z-jVop",
        "outputId": "a977593d-bd93-4954-fd3b-ae77467b5666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize the same model architecture\n",
        "os.chdir(dataset_path)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.load_state_dict(torch.load(\"internship_selection_model.pt\", map_location=torch.device('cpu')))\n",
        "model.to('cpu')\n",
        "model.eval()\n",
        "print(\"✅ Model loaded from 'internship_selection_model.pt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XdEicGJjLLb",
        "outputId": "1500ee3c-6748-4373-a734-41297917ba64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded from 'internship_selection_model.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2 = {\n",
        "    'town': 'St. Johns',\n",
        "    'identify_as': 'Male',\n",
        "    'specify_below': ' ',\n",
        "    \"why_internship\": \"I have an interest in tech and hope to have a tech profession in the future. This opportunity will help me solidify that dream.\",\n",
        "    \"tech_experience\": \"I am able to research online, use Google Sheets, and word for school work. I am also play games on PlayStation.\",\n",
        "    \"non_tech_experience\": \"I am a dedicated volunteer in my community. I come from a larger size family where team work makes the dream work where I am the second eldest so a lot of responsibilities fall onto me. I have played on many all star soccer teams and school teams all throughout my time in school and I am personable and approachable and eager to learn.\",\n",
        "    \"goals\": \"I will be attending MUN in September for engineering.\",\n",
        "    \"other_comments\": \"I am excited for this opportunity.\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "Qki_vWPejwfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the full list of fields you want to clean\n",
        "fields_to_clean = [\n",
        "    'town', 'identify_as', 'specify_below',\n",
        "    'why_internship', 'tech_experience', 'non_tech_experience', 'goals', 'other_comments'\n",
        "]\n",
        "\n",
        "# Clean all selected fields in the sample\n",
        "cleaned_sample = {\n",
        "    key: clean_and_lemmatize(value) if key in fields_to_clean else value\n",
        "    for key, value in sample_2.items()\n",
        "}\n",
        "\n",
        "# Combine all fields into one text string for the model\n",
        "def combine_fields(entry):\n",
        "    return \" \".join([entry.get(col, \"\") for col in fields_to_clean])\n",
        "\n",
        "test_samples = [combine_fields(cleaned_sample)]\n",
        "\n",
        "# Tokenize and move to device\n",
        "inputs = tokenizer(test_samples, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Run model prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "# Display result\n",
        "for i, raw_text in enumerate(test_samples):\n",
        "    label = \"✅ SELECTED\" if preds[i].item() == 1 else \"❌ NOT SELECTED\"\n",
        "    confidence = probs[i][preds[i]].item()\n",
        "    print(f\"\\n➡️ Prediction: {label} (Confidence: {confidence:.2f})\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwZHLCM6jytT",
        "outputId": "7e114ac3-9373-46d2-b700-3da37c231ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "➡️ Prediction: ❌ NOT SELECTED (Confidence: 0.89)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgQPAC6mkDPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}